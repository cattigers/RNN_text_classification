{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "\n",
    "def np_to_tfrecords(X, Y, file_path_prefix, verbose=True):\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=value.reshape(-1)))\n",
    "    # Generate tfrecord writer\n",
    "    result_tf_file = file_path_prefix + '.tfrecords'\n",
    "    writer = tf.python_io.TFRecordWriter(result_tf_file)\n",
    "    if verbose:\n",
    "        print (\"Serializing {:d} examples into {}\".format(X.shape[0], result_tf_file))        \n",
    "    # iterate over each sample,\n",
    "    # and serialize it as ProtoBuf.\n",
    "    for idx in range(X.shape[0]):\n",
    "        #pdb.set_trace()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'X': _int64_feature(X[idx]),\n",
    "        'Y': _int64_feature(Y[idx])}))        \n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)    \n",
    "    if verbose:\n",
    "        print (\"Writing {} done!\".format(result_tf_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 50\n",
    "n_words = None\n",
    "MAX_LABEL = 15\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinhngx/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-4-16b9a9a4f39e>:3: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From /home/vinhngx/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:78: load_dbpedia (from tensorflow.contrib.learn.python.learn.datasets.text_datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "See contrib/learn/README.md\n",
      "WARNING:tensorflow:From /home/vinhngx/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py:56: maybe_download_dbpedia (from tensorflow.contrib.learn.python.learn.datasets.text_datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "See contrib/learn/README.md\n",
      "WARNING:tensorflow:From /home/vinhngx/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py:73: load_csv_without_header (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.data instead.\n",
      "Shuffling data set...\n",
      "Done!\n",
      "Train data size: (560000,)\n",
      "Test data size: (70000,)\n",
      "WARNING:tensorflow:From <ipython-input-4-16b9a9a4f39e>:23: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/vinhngx/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/vinhngx/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Total words: 822383\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and testing data\n",
    "dbpedia = tf.contrib.learn.datasets.load_dataset(\n",
    "'dbpedia', size='large', test_with_fake_data=False)\n",
    "\n",
    "print(\"Shuffling data set...\")\n",
    "x_train = dbpedia.train.data[:, 1]\n",
    "y_train = dbpedia.train.target\n",
    "s = np.arange(len(y_train))\n",
    "np.random.shuffle(s)\n",
    "x_train = x_train[s]\n",
    "y_train = y_train[s]\n",
    "print(\"Done!\")  \n",
    "\n",
    "x_train = pandas.Series(x_train)\n",
    "y_train = pandas.Series(y_train)\n",
    "x_test = pandas.Series(dbpedia.test.data[:, 1])\n",
    "y_test = pandas.Series(dbpedia.test.target)\n",
    "\n",
    "print('Train data size:', x_train.shape)\n",
    "print('Test data size:', x_test.shape)\n",
    "# Process vocabulary\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "MAX_DOCUMENT_LENGTH)\n",
    "\n",
    "x_transform_train = vocab_processor.fit_transform(x_train)\n",
    "x_transform_test = vocab_processor.transform(x_test)\n",
    "\n",
    "x_train_fit = np.array(list(x_transform_train))\n",
    "x_test_fit = np.array(list(x_transform_test))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)\n",
    "\n",
    "\n",
    "y_train = np.expand_dims(np.asarray(y_train), axis=1) \n",
    "y_test = np.expand_dims(np.asarray(y_test), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_fit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing 560000 examples into word-train.tfrecords\n",
      "Writing word-train.tfrecords done!\n",
      "Serializing 70000 examples into word-test.tfrecords\n",
      "Writing word-test.tfrecords done!\n"
     ]
    }
   ],
   "source": [
    "np_to_tfrecords(x_train_fit, np.asarray(y_train, np.int), 'word-train', verbose=True)\n",
    "np_to_tfrecords(x_test_fit, np.asarray(y_test, np.int), 'word-test', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "560000it [01:00, 9310.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Error: 0.000000\n",
      "Test set Error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_err = 0\n",
    "err = 0\n",
    "for i, serialized_example in tqdm(enumerate(tf.python_io.tf_record_iterator('word-train.tfrecords'))):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "    x_1 = np.array(example.features.feature['X'].int64_list.value)\n",
    "    y_1 = np.array(example.features.feature['Y'].int64_list.value)\n",
    "    err += np.linalg.norm(x_train_fit[i]-x_1) + np.linalg.norm(y_train[i]-y_1)\n",
    "    total_err += err\n",
    "    if err>0:\n",
    "        pass\n",
    "        #break\n",
    "print('Train set Error: %f'% total_err)\n",
    "\n",
    "err = 0\n",
    "for i, serialized_example in enumerate(tf.python_io.tf_record_iterator('word-test.tfrecords')):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "    x_1 = np.array(example.features.feature['X'].int64_list.value)\n",
    "    y_1 = np.array(example.features.feature['Y'].int64_list.value)\n",
    "    err += np.linalg.norm(x_test_fit[i]-x_1) + np.linalg.norm(y_test[i]-y_1) \n",
    "    \n",
    "print('Test set Error: %f'% err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.03s/it]\u001b[A\n",
      "2it [00:01,  1.31it/s]\u001b[A\n",
      "3it [00:02,  1.47it/s]\u001b[A\n",
      "100it [00:54,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_err = 0\n",
    "err = 0\n",
    "keys_to_features = {\n",
    "    'X': tf.FixedLenFeature(shape=[MAX_DOCUMENT_LENGTH], dtype=tf.int64),\n",
    "    'Y': tf.FixedLenFeature(shape=[1], dtype=tf.int64)            \n",
    "}\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    for i, serialized_example in tqdm(enumerate(tf.python_io.tf_record_iterator('word-train.tfrecords'))):\n",
    "        #example = tf.train.Example()\n",
    "        #example.ParseFromString(serialized_example)\n",
    "        parsed = tf.parse_single_example(serialized_example, keys_to_features)\n",
    "        parsed = sess.run([parsed])[0]\n",
    "        x_1 = np.array(parsed['X'])\n",
    "        y_1 = np.array(parsed['Y'])\n",
    "        err += np.linalg.norm(x_train_fit[i]-x_1) + np.linalg.norm(y_train[i]-y_1)\n",
    "        total_err += err\n",
    "        if i==100:            \n",
    "            break\n",
    "            \n",
    "print('Train set Error: %f'% total_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.06s/it]\u001b[A\n",
      "2it [00:01,  1.30it/s]\u001b[A\n",
      "3it [00:02,  1.49it/s]\u001b[A\n",
      "4it [00:02,  1.60it/s]\u001b[A\n",
      "5it [00:03,  1.63it/s]\u001b[A\n",
      "6it [00:03,  1.65it/s]\u001b[A\n",
      "7it [00:04,  1.68it/s]\u001b[A\n",
      "8it [00:04,  1.71it/s]\u001b[A\n",
      "9it [00:05,  1.68it/s]\u001b[A\n",
      "10it [00:05,  1.70it/s]\u001b[A\n",
      "11it [00:06,  1.71it/s]\u001b[A\n",
      "12it [00:06,  1.73it/s]\u001b[A\n",
      "13it [00:07,  1.74it/s]\u001b[A\n",
      "14it [00:07,  1.76it/s]\u001b[A\n",
      "15it [00:08,  1.77it/s]\u001b[A\n",
      "16it [00:09,  1.78it/s]\u001b[A\n",
      "100it [00:50,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "err = 0\n",
    "keys_to_features = {\n",
    "    'X': tf.FixedLenFeature(shape=[MAX_DOCUMENT_LENGTH], dtype=tf.int64),\n",
    "    'Y': tf.FixedLenFeature(shape=[1], dtype=tf.int64)            \n",
    "}\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    for i, serialized_example in tqdm(enumerate(tf.python_io.tf_record_iterator('word-train.tfrecords'))):\n",
    "        #example = tf.train.Example()\n",
    "        #example.ParseFromString(serialized_example)\n",
    "\n",
    "        parsed = tf.parse_single_example(serialized_example, keys_to_features)\n",
    "        parsed = sess.run([parsed])[0]\n",
    "\n",
    "        x_1 = np.array(parsed['X'])\n",
    "        y_1 = np.array(parsed['Y'])\n",
    "        err += np.linalg.norm(x_train_fit[i]-x_1) + np.linalg.norm(y_train[i]-y_1)\n",
    "        total_err += err\n",
    "        if i==100:            \n",
    "            break\n",
    "print('Test set Error: %f'% total_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
